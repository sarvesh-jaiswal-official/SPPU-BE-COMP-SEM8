{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hBi7n7-bmCYJ"
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import sent_tokenize \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SARVESH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pwjpRbmsmOGx"
   },
   "outputs": [],
   "source": [
    "text=\"India is a unique country with diversity. Unity is diversity is the main slogan of the country.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZh0TiOwmUUR",
    "outputId": "dbe1f188-ff66-4d4e-fdca-3b21c6cc388f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['India is a unique country with diversity.', 'Unity is diversity is the main slogan of the country.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0ZK5qPmm1Ew",
    "outputId": "002987f8-5fbf-45f8-fbbd-94c0bacd59aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whitespace tokenization = ['India', 'is', 'a', 'unique', 'country', 'with', 'diversity.', 'Unity', 'is', 'diversity', 'is', 'the', 'main', 'slogan', 'of', 'the', 'country.']\n"
     ]
    }
   ],
   "source": [
    "print(f'whitespace tokenization = {text.split()}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "64RbyPWJm8aR"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dNjGSyEUm-L4",
    "outputId": "0312681e-6f1c-414d-e90e-89296bc2a30d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation-based tokenization = ['India', 'is', 'a', 'unique', 'country', 'with', 'diversity', '.', 'Unity', 'is', 'diversity', 'is', 'the', 'main', 'slogan', 'of', 'the', 'country', '.']\n"
     ]
    }
   ],
   "source": [
    "print(f'Punctuation-based tokenization = {wordpunct_tokenize(text)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0Eb3HQ21nCXo"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "i2AtnkyHnF2J"
   },
   "outputs": [],
   "source": [
    "sentence=\"What's your name?\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GyH8Y92-nKWZ",
    "outputId": "a805de1e-1f2a-4ee7-ee5f-0a35b6323097"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default/Treebank tokenization = ['What', \"'s\", 'your', 'name', '?']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer() \n",
    "print(f'Default/Treebank tokenization = {tokenizer.tokenize(sentence) }') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4unwgn-UnOao",
    "outputId": "494c1936-2b26-4892-a983-aae2b613444d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: emoji in c:\\users\\sarvesh\\appdata\\roaming\\python\\python39\\site-packages (2.2.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install emoji --upgrade "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4uPJWUG-nRug"
   },
   "outputs": [],
   "source": [
    "import emoji "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TYjI0tECnXdI",
    "outputId": "9acf5af5-d744-4bc6-bcca-d480c9c69451"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Everyone! ðŸ˜€\n"
     ]
    }
   ],
   "source": [
    "print(emoji.emojize('Hi Everyone! :grinning_face:')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FZEKC_vknYjY"
   },
   "outputs": [],
   "source": [
    "sentence1= emoji.emojize('Hi Everyone! :grinning face:') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "d-Z5T9Z6nrvQ"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iJ3NGKN4nuz5",
    "outputId": "d70eba13-03d4-4d78-e813-e40a8040ba77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet-rules based tokenization = ['Hi', 'Everyone', '!', ':', 'grinning', 'face', ':']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer() \n",
    "print(f'Tweet-rules based tokenization = {tokenizer.tokenize(sentence1)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "oz-hPPefoC5J"
   },
   "outputs": [],
   "source": [
    "sentence2=\"Hope, is the only thing stronger than fear! Hunger Games\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XCaNxNTtoGh5",
    "outputId": "f7632f36-7320-4f1a-98aa-de7cfa6cac50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hope', ',', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear', '!', 'Hunger', 'Games']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader.tagged import word_tokenize\n",
    "print(word_tokenize(sentence2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "1fRqTGn7o81j"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ggn3hOtzo-Np",
    "outputId": "3cc63e74-40db-4342-c73a-bcbddafaef96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-word expression (MWE) tokenization = ['Hope', ',', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear', '!', 'Hunger_Games']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = MWETokenizer() \n",
    "tokenizer.add_mwe(('Hunger', 'Games')) \n",
    "print(f'Multi-word expression (MWE) tokenization = {tokenizer.tokenize(word_tokenize(sentence2) )}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_LXTUtGSpUE3",
    "outputId": "c03d711d-e963-4599-c731-7302494f4490"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fairli\n"
     ]
    }
   ],
   "source": [
    "# Import the toolkit and the full Porter Stemmer library \n",
    "import nltk \n",
    "from nltk.stem.porter import * \n",
    "p_stemmer = PorterStemmer() \n",
    "words = ['run','runner','running','ran','runs','easily','fairly'] \n",
    "for word in words: \n",
    "  print(word+' --> '+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o5h6ACVGpsjA",
    "outputId": "ad950769-581d-4f93-a91c-f0061ae4c716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fair\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer \n",
    "# The Snowball Stemmer requires that you pass a language parameter \n",
    "s_stemmer = SnowballStemmer(language='english') \n",
    "words = ['run','runner','running','ran','runs','easily','fairly'] \n",
    "for word in words: \n",
    "  print(word+' --> '+s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "VvXuAaeNqBh1"
   },
   "outputs": [],
   "source": [
    "#Perform standard imports: \n",
    "import spacy \n",
    "# Load English tokenizer, tagger, parser and NER \n",
    "nlp = spacy.load('en_core_web_sm') \n",
    "def show_lemmas(text): \n",
    "  for token in text: \n",
    "    print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XI7DxJXhrBzx",
    "outputId": "8c70b35a-b35a-4267-cfa8-a812eac49188"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            PRON   4690420944186131903    I\n",
      "am           AUX    10382539506755952630   be\n",
      "a            DET    11901859001352538922   a\n",
      "runner       NOUN   12640964157389618806   runner\n",
      "running      VERB   12767647472892411841   run\n",
      "in           ADP    3002984154512732771    in\n",
      "a            DET    11901859001352538922   a\n",
      "race         NOUN   8048469955494714898    race\n",
      "because      SCONJ  16950148841647037698   because\n",
      "I            PRON   4690420944186131903    I\n",
      "love         VERB   3702023516439754181    love\n",
      "to           PART   3791531372978436496    to\n",
      "run          VERB   12767647472892411841   run\n",
      "since        SCONJ  10066841407251338481   since\n",
      "I            PRON   4690420944186131903    I\n",
      "ran          VERB   12767647472892411841   run\n",
      "today        NOUN   11042482332948150395   today\n",
      ".            PUNCT  12646065887601541794   .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"I am a runner running in a race because I love to run since I ran today.\")\n",
    "show_lemmas (doc) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
